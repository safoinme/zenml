{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO_v5iIaYFi2"
   },
   "source": [
    "# ZenML Quickstart Guide\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/notebooks/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "This quickstart aims to help you get your first practical experience with ZenML and give you a brief overview of various functionalities. \n",
    "\n",
    "Throughout this quickstart, we will:\n",
    "- Train a model, evaluate it, deploy it, and embed it in an inference pipeline,\n",
    "- Automatically version, track, and cache data, models, and other artifacts,\n",
    "- Track model hyperparameters and metrics in an experiment tracking tool,\n",
    "- Measure and visualize train-test skew, training-serving skew, and data drift.\n",
    "\n",
    "**New to MLOps?** Then you might want to start with our [**ZenBytes**](https://github.com/zenml-io/zenbytes) lesson series instead, where we cover each MLOps concept in much more detail. This quickstart assumes you are already familiar with basic MLOps issues and just want to learn how to approach them with ZenML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VNLEesHEyjkg"
   },
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install zenml matplotlib  # install ZenML and matplotlib\n",
    "!zenml integration install dash sklearn mlflow evidently facets -y  # install ZenML integrations\n",
    "!zenml init  # Initialize a ZenML repository\n",
    "!zenml profile create quickstart  # create a new ZenML profile\n",
    "!zenml profile set quickstart  # use the new ZenML profile for all runs\n",
    "%pip install pyparsing==2.4.2  # required for Colab\n",
    "\n",
    "import IPython\n",
    "\n",
    "# automatically restart kernel\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait for the installation to complete before running subsequent cells. At the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Google Colab\n",
    "\n",
    "If you follow this quickstart in Google's Colab, you will need an [ngrok account](https://dashboard.ngrok.com/signup) to view some of the visualizations later. Please set up an account, then set your user token below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"\"  # TODO: set your ngrok token if you are working on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLAB ONLY setup\n",
    "try:\n",
    "    import google.colab\n",
    "\n",
    "    IN_COLAB = True\n",
    "\n",
    "    # install ngrok and set auth token\n",
    "    !pip install pyngrok\n",
    "    !ngrok authtoken {NGROK_TOKEN}\n",
    "\n",
    "except ModuleNotFoundError as err:\n",
    "    IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an MLOps Stack\n",
    "\n",
    "ZenML decouples your code from the infrastructure and tooling you use.\n",
    "This enables you to quickly take your code from experimentation to production.\n",
    "Furthermore, using ZenML prevents vendor lock-in by allowing you to switch out any part of your MLOps stack easily\n",
    "See the [ZenML Integrations](https://docs.zenml.io/features/integrations) page for a list of all tools we currently support.\n",
    "\n",
    "Throughout this quickstart, we will use the following MLOps stack: [MLFlow](https://mlflow.org/) for experiment tracking and model deployment, [Facets](https://pair-code.github.io/facets/) for visualizing train-test skew and training-serving skew, and [Evidently](https://evidentlyai.com/) for data drift detection.\n",
    "\n",
    "![Quickstart MLOps Stack Overview](../_assets/stack_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we need to register all stack components that require configuration into our ZenML MLOps stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow_tracker --flavor=mlflow\n",
    "\n",
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow_deployer --flavor=mlflow\n",
    "\n",
    "# Add the MLflow components into our default stack\n",
    "!zenml stack update default -d mlflow_deployer -e mlflow_tracker\n",
    "\n",
    "# Visualize the current ZenML stack\n",
    "!zenml stack describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define ML Pipelines\n",
    "Let us now use ZenML to write two ML pipelines for continuous training and serving.\n",
    "\n",
    "The training pipeline will:\n",
    "- Load the [Digits dataset](https://paperswithcode.com/dataset/digits),\n",
    "- Train a model on the training data (and track hyperparameters using [MLFlow](https://mlflow.org/)),\n",
    "- Test the model on the test data,\n",
    "- Deploy the model (with [MLFlow](https://mlflow.org/)) if the test accuracy is higher than a certain threshold,\n",
    "- Compare train and test data for later skew visualization (with [Facets](https://pair-code.github.io/facets/)).\n",
    "\n",
    "The inference pipeline will:\n",
    "- Load inference data,\n",
    "- Load the most recently deployed model,\n",
    "- Run model inference on the inference data,\n",
    "- Compare train and inference data for later skew visualization (with [Facets](https://pair-code.github.io/facets/)),\n",
    "- Check for data drift (with [Evidently](https://evidentlyai.com/)).\n",
    "\n",
    "You can see a visualization of the two pipelines below:\n",
    "\n",
    "![Overview of Quickstart Pipelines](../_assets/quickstart_pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define those pipelines with ZenML. To do so, we simply write a Python function that defines how the data will move through the different steps and decorate it with ZenML's `@pipeline` decorator. Under the hood, ZenML will build a [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph) that determines the order in which the steps need to be executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.pipelines import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=False)\n",
    "def training_pipeline(\n",
    "    training_data_loader,\n",
    "    skew_comparison,\n",
    "    trainer,\n",
    "    evaluator,\n",
    "    deployment_trigger,\n",
    "    model_deployer,\n",
    "):\n",
    "    \"\"\"Train, evaluate, and deploy a model.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = training_data_loader()\n",
    "    skew_comparison(X_train, X_test)\n",
    "    model = trainer(X_train=X_train, y_train=y_train)\n",
    "    test_acc = evaluator(X_test=X_test, y_test=y_test, model=model)\n",
    "    deployment_decision = deployment_trigger(test_acc)\n",
    "    model_deployer(deployment_decision, model)\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def inference_pipeline(\n",
    "    inference_data_loader,\n",
    "    prediction_service_loader,\n",
    "    predictor,\n",
    "    training_data_loader,\n",
    "    skew_comparison,\n",
    "    drift_detector,\n",
    "):\n",
    "    \"\"\"Inference pipeline with data drift detection.\"\"\"\n",
    "    inference_data = inference_data_loader()\n",
    "    model_deployment_service = prediction_service_loader()\n",
    "    predictor(model_deployment_service, inference_data)\n",
    "    training_data, _, _, _ = training_data_loader()\n",
    "    reference, comparison = skew_comparison(training_data, inference_data)\n",
    "    drift_detector(reference, comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Pipeline Steps\n",
    "\n",
    "Next, we need to implement the steps that make up these pipelines. \n",
    "Again, we can do this by writing simple Python functions and decorating them with ZenML's `@step` decorator.\n",
    "\n",
    "In total, we will need ten steps:\n",
    "- Training data loader\n",
    "- Inference data loader\n",
    "- Model trainer\n",
    "- Model evaluator\n",
    "- Deployment trigger\n",
    "- Model deployer\n",
    "- Prediction service loader\n",
    "- Predictor\n",
    "- Skew comparison\n",
    "- Drift detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loaders\n",
    "Let's start with data loading. We load the Digits dataset for training and, for simplicity, use some random noise samples for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from zenml.integrations.sklearn.helpers.digits import get_digits\n",
    "from zenml.steps import step, Output\n",
    "\n",
    "\n",
    "@step\n",
    "def training_data_loader() -> Output(\n",
    "    X_train=np.ndarray, X_test=np.ndarray, y_train=np.ndarray, y_test=np.ndarray\n",
    "):\n",
    "    \"\"\"Loads the digits dataset as normal numpy arrays.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = get_digits()\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def inference_data_loader() -> np.ndarray:\n",
    "    \"\"\"Load some (random) inference data.\"\"\"\n",
    "    return np.random.rand(1, 64)  # flattened 8x8 random noise image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Trainer\n",
    "To train our model, we define a step that builds an [sklearn SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) model and fits it on the given training data. Additionally, we log all model hyperparameters and metrics to [MLFlow](https://mlflow.org/).\n",
    "\n",
    "Note that we do not need to save the model within the step explicitly; ZenML is automatically taking care of this for us. Under the hood, ZenML persists all step inputs and outputs in an [Artifact Store](https://docs.zenml.io/core-concepts#artifact-store) and their metadata in a [Metadata Store](https://docs.zenml.io/core-concepts#metadata-store). This also means that all of our data and models are automatically versioned and tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.svm import SVC\n",
    "from zenml.integrations.mlflow.mlflow_step_decorator import enable_mlflow\n",
    "\n",
    "\n",
    "@enable_mlflow  # setup MLflow\n",
    "@step(enable_cache=False)\n",
    "def svc_trainer_mlflow(\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn SVC classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hparams and metrics to MLflow\n",
    "    model = SVC(gamma=0.001)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluator and Deployment Trigger\n",
    "\n",
    "Since our model is a [sklearn Model](https://scikit-learn.org/stable/developers/develop.html), we can simply call `model.score` to compute its test accuracy.\n",
    "\n",
    "We then use the output of this step to only trigger deployment for models that achieved >90% test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def evaluator(\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    "    model: ClassifierMixin,\n",
    ") -> float:\n",
    "    \"\"\"Calculate the accuracy on the test set\"\"\"\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc}\")\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def deployment_trigger(test_acc: float) -> bool:\n",
    "    \"\"\"Only deploy if the test accuracy > 90%.\"\"\"\n",
    "    return test_acc > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Deployment and Drift Detection\n",
    "\n",
    "ZenML provides default steps for MLflow model deployment and Evidently drift detection, which we can simply import:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps import mlflow_model_deployer_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.evidently.steps import (\n",
    "    EvidentlyProfileConfig,\n",
    "    EvidentlyProfileStep,\n",
    ")\n",
    "\n",
    "evidently_profile_config = EvidentlyProfileConfig(\n",
    "    column_mapping=None, profile_sections=[\"datadrift\"]\n",
    ")\n",
    "\n",
    "drift_detector = EvidentlyProfileStep(config=evidently_profile_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew Comparison\n",
    "\n",
    "For the Facets skew comparison, we need to write a step that outputs two pandas DataFrames that should be compared for skew. Since the data loaders load all data in numpy array format, we simply need to write a function that converts from numpy to pandas here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def skew_comparison(\n",
    "    reference_input: np.ndarray,\n",
    "    comparison_input: np.ndarray,\n",
    ") -> Output(reference=pd.DataFrame, comparison=pd.DataFrame):\n",
    "    \"\"\"Convert data from numpy to pandas for skew comparison.\"\"\"\n",
    "    columns = [str(x) for x in list(range(reference_input.shape[1]))]\n",
    "    return pd.DataFrame(reference_input, columns=columns), pd.DataFrame(\n",
    "        comparison_input, columns=columns\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Service Loader and Predictor\n",
    "\n",
    "Lastly, we need to write the inference pipeline steps for loading a deployed model and computing its prediction on the test data.\n",
    "\n",
    "To load the deployed model, we query ZenML's artifact store to find a model deployed with our current MLOps stack and the given training pipeline and deployment step names (more on this later):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.services import BaseService\n",
    "from zenml.repository import Repository\n",
    "\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> BaseService:\n",
    "    \"\"\"Load the model service of our train_evaluate_deploy_pipeline.\"\"\"\n",
    "    repo = Repository(skip_repository_check=True)\n",
    "    model_deployer = repo.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"training_pipeline\",\n",
    "        pipeline_step_name=\"mlflow_model_deployer_step\",\n",
    "        running=True,\n",
    "    )\n",
    "    service = services[0]\n",
    "    return service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inference the deployed model, we simply call its `predict()` method to get logits and compute the `argmax` to obtain the final prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    data: np.ndarray,\n",
    ") -> Output(predictions=list):\n",
    "    \"\"\"Run a inference request against a prediction service\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    prediction = service.predict(data)\n",
    "    prediction = prediction.argmax(axis=-1)\n",
    "    print(f\"Prediction is: {[prediction.tolist()]}\")\n",
    "    return [prediction.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ZenML Pipelines\n",
    "\n",
    "Running pipelines is as simple as calling the `run()` method on an instance of the defined pipeline.\n",
    "\n",
    "**Note:** If you get an error `NoSuchProcess: process no longer exists (pid=95685)` when running the following cell, simply run the cell again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline(\n",
    "    training_data_loader=training_data_loader(),\n",
    "    skew_comparison=skew_comparison(),\n",
    "    trainer=svc_trainer_mlflow(),\n",
    "    evaluator=evaluator(),\n",
    "    deployment_trigger=deployment_trigger(),\n",
    "    model_deployer=mlflow_model_deployer_step(),\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training pipeline run has finished, the trained model will have been deployed using MLflow and is ready for prediction requests. We can use `zenml served-models list` to get an overview of all currently deployed models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml served-models list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run the inference pipeline, the `prediction_service_loader` step will fetch a reference to the deployed model so further steps can send prediction requests to it. If we were to rerun the training pipeline, we would automatically overwrite the deployed model, so we can deploy new models to production without having to interrupt or modify the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline(\n",
    "    inference_data_loader=inference_data_loader(),\n",
    "    prediction_service_loader=prediction_service_loader(),\n",
    "    predictor=predictor(),\n",
    "    training_data_loader=training_data_loader(),\n",
    "    skew_comparison=skew_comparison(),\n",
    "    drift_detector=drift_detector,\n",
    ").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Visualization & Caching\n",
    "\n",
    "After we have run a pipeline, we can visualize it using ZenML's Dash integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer import (\n",
    "    PipelineRunLineageVisualizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, let's load the ZenML repository, which is where all our pipelines live. We can then fetch a specific pipeline by name using the `get_pipeline()` method. The pipeline's name defaults to the function name, if not specified. Afterward, we can get a specific pipeline run using the pipeline's `get_run()` method. Alternatively, we can simply access the latest run of a pipeline using `runs[-1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.repository import Repository\n",
    "\n",
    "repo = Repository()\n",
    "\n",
    "# get latest training pipeline run\n",
    "train_run = repo.get_pipeline(pipeline_name=\"training_pipeline\").runs[-1]\n",
    "\n",
    "# get latest inference pipeline run\n",
    "inference_run = repo.get_pipeline(pipeline_name=\"inference_pipeline\").runs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize a run, we can simply use `PipelineRunLineageVisualizer().visualize()` now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pipeline(run):\n",
    "    if IN_COLAB:\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(8050)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "\n",
    "    PipelineRunLineageVisualizer().visualize(run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(train_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now see an interactive visualization in your browser. The squares represent your artifacts and the circles your pipeline steps. Also, note that the different nodes are color-coded. Right now, they should all be blue, meaning all steps were executed correctly. \n",
    "If a pipeline were to fail or run for too long, you could find the responsible step marked red or yellow.\n",
    "Lastly, if you rerun any of the pipelines with a different `run_name`, you will notice that several nodes will change from blue to green, which indicates they were still cached from the last run and did not have to be rerun. This means we can switch out intermediate steps of our pipeline without rerunning any of the previous steps, which can save us a lot of time and resources in real production environments!\n",
    "\n",
    "<img src=\"../_assets/train_pipeline.png\" alt=\"Training Pipeline Visualization\" width=\"50%\"/>\n",
    "\n",
    "Let's also visualize our inference pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_pipeline(inference_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../_assets/inference_pipeline.png\" alt=\"Inference Pipeline Visualization\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data Skew and Data Drift\n",
    "\n",
    "ZenML provides a variety of visualization tools in addition to the pipeline visualizer shown above. E.g., using the `FacetStatisticsVisualizer` we can visualize differences between data distributions to check for train-test or training-serving skew, and using the `EvidentlyVisualizer` we can visualize data drift:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.facets.visualizers.facet_statistics_visualizer import (\n",
    "    FacetStatisticsVisualizer,\n",
    ")\n",
    "from zenml.integrations.evidently.visualizers import EvidentlyVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_skew_step = train_run.get_step(name=\"skew_comparison\")\n",
    "FacetStatisticsVisualizer().visualize(train_test_skew_step, magic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In some environments the FacetsVisualizer needs to be run twice to get up running. If you executed the previous cell and nothing happens, just try to run the cell another time.\n",
    "\n",
    "As we see, the train and test data distributions look very similar, meaning we have minimal train-test skew.\n",
    "\n",
    "<img src=\"../_assets/train_test_skew.png\" alt=\"Train-Test Skew Visualization with Facets\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_serving_skew_step = inference_run.get_step(name=\"skew_comparison\")\n",
    "FacetStatisticsVisualizer().visualize(training_serving_skew_step, magic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing training and serving data, we can see quite some skew. This is, of course, expected since we mocked our inference data loader using random data. Therefore, the visualizations probably also look different for you than the example below.\n",
    "\n",
    "<img src=\"../_assets/training_serving_skew.png\" alt=\"Training-Serving Skew Visualization with Facets\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_detection_step = inference_run.get_step(name=\"drift_detector\")\n",
    "EvidentlyVisualizer().visualize(drift_detection_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accordingly, Evidently will also detect data drift for several features:\n",
    "\n",
    "<img src=\"../_assets/data_drift.png\" alt=\"Evidently Data Drift Visualization\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow Experiment Tracking\n",
    "\n",
    "Lastly, remember how we added MLflow experiment tracking to our `svc_trainer_mlflow` step before?\n",
    "Those two simple lines of code automatically configured and initialized MLflow and logged all hyperparameters and metrics there.\n",
    "\n",
    "Let's start up the MLflow UI and check it out!\n",
    "\n",
    "![MLflow UI](../_assets/mlflow_ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "\n",
    "def open_mlflow_ui(port=4997):\n",
    "    if IN_COLAB:\n",
    "        from pyngrok import ngrok\n",
    "\n",
    "        public_url = ngrok.connect(port)\n",
    "        print(f\"\\x1b[31mIn Colab, use this URL instead: {public_url}!\\x1b[0m\")\n",
    "\n",
    "    !mlflow ui --backend-store-uri=\"{get_tracking_uri()}\" --port={port}\n",
    "\n",
    "\n",
    "open_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just built your first ML Pipelines! You not only trained a model, you also deployed it, served it, and learned how to monitor and visualize everything that's going on. Did you notice how easy it was to bring all of the different components together using ZenML's abstractions? And that is just the tip of the iceberg of what ZenML can do; check out the [**Integrations**](https://docs.zenml.io/features/integrations) page for a list of all the cool MLOps tools that ZenML supports!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "* If you have questions or feedback... \n",
    "  * Join our [**Slack Community**](https://zenml.io/slack-invite) and become part of the ZenML family!\n",
    "* If this quickstart was a bit too quick for you... \n",
    "  * Check out [**ZenBytes**](https://github.com/zenml-io/zenbytes), our lesson series on practical MLOps, where we cover each MLOps concept in much more detail.\n",
    "* If you want to learn more about using or extending ZenML...\n",
    "  * Check out our [**Docs**](https://docs.zenml.io/) or read through our code on [**Github**](https://github.com/zenml-io/zenml).\n",
    "* If you want to quickly learn how to use a specific tool with ZenML...\n",
    "  * Check out our collection of [**Examples**](https://github.com/zenml-io/zenml/tree/doc/hamza-misc-updates/examples).\n",
    "* If you want to see some advanced ZenML use cases... \n",
    "  * Check out [**ZenFiles**](https://github.com/zenml-io/zenfiles), our collection of production-grade ML use-cases."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ZenML Quickstart.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "21cf70ddc64093920bb77049cccd3605bf48f1aba2eda25e87372ff093b94529"
  },
  "kernelspec": {
   "display_name": "zenml-quickstart",
   "language": "python",
   "name": "zenml-quickstart"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
