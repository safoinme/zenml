---
description: Execute individual steps in specialized environments.
---

# Run steps in specialized environments with step operators

The step operator defers the execution of individual steps in a pipeline to
specialized runtime environments that are
optimized for Machine Learning workloads. This is helpful when there is a
requirement for specialized cloud backends
for different steps. One example could be using powerful GPU instances for
training jobs or distributed compute for
ingestion streams.

While an orchestrator defines how and where your entire pipeline runs, a step
operator defines how and where an individual step runs. 

An operator can be registered as follows:

```bash
zenml step-operator register OPERATOR_NAME \
    --flavor=OPERATOR_TYPE
    ...
```

```python
from zenml.steps import step

@step(custom_step_operator=OPERATOR_NAME)
def trainer(...) -> ...:
    """Train a model"""
    # This step will run in environment specified by operator
```

## Pre-built Step Operators

ZenML has support for some pre-built step operators, namely:

- AWS Sagemaker
- AzureML
- Vertex AI

These operators are all to be used for the use-case of running a training job on
specialized cloud backends.

{% tabs %}
{% tab title="AzureML" %}

* First, you require
  a `Machine learning` [resource on Azure](https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources)
  .
  If you don't already have one, you can create it through the `All resources`
  page on the Azure portal.
* Once your resource is created, you can head over to the `Azure Machine
  Learning Studio`
  and [create a compute cluster](https://docs.microsoft.com/en-us/azure/machine-learning/quickstart-create-resources#cluster)
  to run your pipelines.
* Next, you will need an `environment` for your pipelines. You can simply
  create one following the
  guide [here](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-environments-in-studio)
  .
* Finally, we have to set up our artifact store. In order to do this, we need
  to [create a blob container](https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-portal)
  on Azure.

Optionally, you can also create
a [Service Principal for authentication](https://docs.microsoft.com/en-us/azure/developer/java/sdk/identity-service-principal-auth)
.
This might especially be useful if you are planning to orchestrate your
pipelines in a non-local setup such as Kubeflow where your local authentication
won't be accessible.

The command to register the stack component would look like the following. More
details about the parameters that you
can configure can be found in the class definition of Azure Step Operator in the
API docs (https://apidocs.zenml.io/).

```bash
zenml step-operator register azureml \
    --flavor=azureml \
    --subscription_id=<AZURE_SUBSCRIPTION_ID> \
    --resource_group=<AZURE_RESOURCE_GROUP> \
    --workspace_name=<AZURE_WORKSPACE_NAME> \
    --compute_target_name=<AZURE_COMPUTE_TARGET_NAME> \
    --environment_name=<AZURE_ENVIRONMENT_NAME> 
```

{% endtab %}

{% tab title="Amazon SageMaker" %}

* First, you need to create a role in the IAM console that you want the jobs
  running in Sagemaker to assume. This role
  should at least have the `AmazonS3FullAccess` and `AmazonSageMakerFullAccess`
  policies applied.
  Check [this link](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html#sagemaker-roles-create-execution-role)
  to learn how to create a role.

* Next, you need to choose what instance type needs to be used to run your jobs.
  You can get the
  list [here](https://docs.aws.amazon.com/sagemaker/latest/dg/notebooks-available-instance-types.html)
  .

* Optionally, you can choose an S3 bucket to which Sagemaker should output any
  artifacts from your training run.

* You can also supply an experiment name if you have one created already.
  Check [this guide](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html)
  to know how. If not
  provided, the job runs would be independent.

* You can also choose a custom docker image that you want ZenML to use as a base
  image for creating an environment to
  run your jobs in Sagemaker.

* You need to have the `aws` cli set up with the right credentials. Make sure
  you have the permissions to create and
  manage Sagemaker runs.

* A container registry has to be configured in the stack. This registry will be
  used by ZenML to push your job images
  that Sagemaker will run.

Once you have all these values handy, you can proceed to setting up the
components required for your stack.

The command to register the stack component would look like the following. More
details about the parameters that you
can configure can be found in the class definition of Sagemaker Step Operator in
the API
docs (https://apidocs.zenml.io/).

```bash
zenml step-operator register sagemaker \
    --flavor=sagemaker
    --role=<SAGEMAKER_ROLE> \
    --instance_type=<SAGEMAKER_INSTANCE_TYPE>
    --base_image=<CUSTOM_BASE_IMAGE>
    --bucket_name=<S3_BUCKET_NAME>
    --experiment_name=<SAGEMAKER_EXPERIMENT_NAME>
```

{% endtab %}

{% tab title="GCP Vertex AI" %}

* You need to have the `gcp` cli set up with the right credentials. Make sure
  you have the permissions to create and
  manage Vertex AI custom jobs. Preferably, you should create
  a [service account](https://cloud.google.com/iam/docs/service-accounts) with
  the right permissions to create Vertex AI
  jobs (`roles/aiplatform.admin`) and push to the Artifact/Container
  registry (`roles/(roles/storage.admin`). Then set
  the `GOOGLE_APPLICATION_CREDENTIALS` env variable to point to the service
  account file.

* Next, you need to choose what instance type needs to be used to run your jobs.
  You can get the
  list [here]( https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types)
  .

* You can choose
  a [GCP bucket](https://cloud.google.com/storage/docs/creating-buckets) to
  which Vertex should output
  any artifacts from your training run.

* You can also choose a custom docker image that you want ZenML to use as a base
  image for creating an environment to
  run your jobs on Vertex AI.

* A container registry has to be configured in the stack. This registry will be
  used by ZenML to push your job images
  that Vertex will use. Check out the [cloud guide](guide-aws-gcp-azure.md) to
  learn how you can set up an GCP container
  registry.

Once you have all these values handy, you can proceed to setting up the
components required for your stack.

The command to register the stack component would look like the following. More
details about the parameters that you
can configure can be found in the class definition of Vertex Step Operator in
the API docs (https://apidocs.zenml.io/).

```bash
zenml step-operator register vertex \
    --flavor=vertex \
    --project=zenml-core \
    --service_account_path=... \
    --region=europe-west1 \
    --machine_type=n1-standard-4 \
    --base_image=<CUSTOM_BASE_IMAGE> \
    --accelerator_type=...
```

{% endtab %}
{% endtabs %}

A concrete example of using these step operators can be found 
[here](https://github.com/zenml-io/zenml/tree/main/examples/step_operator_remote_training)

## Building your own StepOperator

To have ZenML run your steps in your own backend, all you need to do is
implement
the [BaseStepOperator](https://apidocs.zenml.io) class with the code that sets
up your environment and submits the ZenML
entrypoint command to it.

```python
from abc import  ABC, abstractmethod
from typing import List
from zenml.stack import StackComponent

class BaseStepOperator(StackComponent, ABC):
    """Base class for all ZenML step operators."""

    ...
    ...


@abstractmethod
def launch(
        self,
        pipeline_name: str,
        run_name: str,
        requirements: List[str],
        entrypoint_command: List[str],
) -> None:
    """Abstract method to execute a step.
    Concrete step operator subclasses must implement the following
    functionality in this method:
    - Prepare the execution environment and install all the necessary
      `requirements`
    - Launch a **synchronous** job that executes the `entrypoint_command`
    Args:
        pipeline_name: Name of the pipeline which the step to be executed
            is part of.
        run_name: Name of the pipeline run which the step to be executed
            is part of.
        entrypoint_command: Command that executes the step.
        requirements: List of pip requirements that must be installed
            inside the step operator environment.
    """
    # Write custom logic here.
```

The launch method is what gets called when ZenML is executing your step and it
is responsible for any logic that
pertains to your custom backend.